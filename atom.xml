<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Alex Chiu的学习空间</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zpqiu.github.io/"/>
  <updated>2019-03-27T04:39:47.945Z</updated>
  <id>https://zpqiu.github.io/</id>
  
  <author>
    <name>Alex Chiu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dual Co-Matching Network for Multi-choice Reading Comprehension笔记</title>
    <link href="https://zpqiu.github.io/Dual%20Co-Matching%20Network%20for%20Multi-choice%20Reading%20Comprehension%E7%AC%94%E8%AE%B0/"/>
    <id>https://zpqiu.github.io/Dual Co-Matching Network for Multi-choice Reading Comprehension笔记/</id>
    <published>2019-03-27T04:38:14.005Z</published>
    <updated>2019-03-27T04:39:47.945Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>目前流行的SQuAD数据集是比较简单的MRC任务，因为已经给定了阅读段落，而且问题的答案已经被完全的包含在了阅读段落里。</p><p>本文针对了一个更难的数据集RACE。RACE数据集的问题答案<strong>不一定</strong>包含在给定的阅读材料中，这需要模型更加深度地了解阅读材料。</p><p><img src="https://user-gold-cdn.xitu.io/2019/3/27/169bcff42d75dfc7?w=2586&amp;h=1190&amp;f=png&amp;s=452272" alt=""></p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>基于BERT来设计的Model， 输入为 P Q A，分别为阅读材料、问题和候选答案。</p><h2 id="Encoding-Layer"><a href="#Encoding-Layer" class="headerlink" title="Encoding Layer"></a>Encoding Layer</h2><p>首先用BERT分别编码P Q 和 A。而不是如BERT一样直接用 [CLS] 的embedding作为表达。</p><script type="math/tex; mode=display">\mathbf{H}^{p}=B E R T(\mathbf{P}), \mathbf{H}^{q}=B E R T(\mathbf{Q})</script><script type="math/tex; mode=display">\mathbf{H}^{a}=BERT(\mathbf{A})</script><h2 id="Matching-Layer"><a href="#Matching-Layer" class="headerlink" title="Matching Layer"></a>Matching Layer</h2><p>分别进行了P和A的attention，以及P和Q的attention。下面是以P和A之间的attention为例的公式。</p><p><strong>Attention</strong></p><p>先计算matching matrix</p><script type="math/tex; mode=display">\mathbf{W}=\operatorname{SoftMax}\left(\mathbf{H}^{p}\left(H^{a} G+b\right)^{T}\right)</script><p>再Attend，得到passage-wise下的answer表达，以及answer-wise下的passage表达</p><script type="math/tex; mode=display">\mathbf{M}^{p}=\mathbf{W} \mathbf{H}^{a}, \mathbf{M}^{a}=\mathbf{W}^{T} \mathbf{H}^{p}</script><p><strong>Fuse</strong></p><p>将attend后的表达与原始的表达进行融合。其中 $\mathbf{S}^{a}$ 和 $\mathbf{S}^{p}$是最终的表达。</p><script type="math/tex; mode=display">\mathbf{S}^{a}=F\left(\left[\mathbf{M}^{a}-\mathbf{H}^{a} ; \mathbf{M}^{a} \cdot \mathbf{H}^{a}\right] W_{1}+b_{1}\right)</script><script type="math/tex; mode=display">\mathbf{S}^{p}=F\left(\left[\mathbf{M}^{p}-\mathbf{H}^{p} ; \mathbf{M}^{p} \cdot \mathbf{H}^{p}\right] W_{2}+b_{2}\right)</script><p>同样地，通过对P和Q之间的attention操作，可以得到 $\mathbf{S}^{q}$ 和 $\mathbf{S}^{p’}$</p><h2 id="Aggregation-Layer"><a href="#Aggregation-Layer" class="headerlink" title="Aggregation Layer"></a>Aggregation Layer</h2><p>这一步是在上一步的结果上做row-wise max pooling，得到每个sequence的表达。</p><p>比如，对$\mathbf{S}^{p}$做max-pooling，得到</p><script type="math/tex; mode=display">\mathbf{C}^{p}=Pooling\left(\mathbf{S}^{p}\right)</script><p>同样的操作，可以得到$\mathbf{C}^{p} ; \mathbf{C}^{a} ; \mathbf{C}^{p^{\prime}} ; \mathbf{C}^{q}$</p><p>将四个vector进行concatenate，就得到了对于一个 {P,Q,A} 三元组的表达 $\mathbf{C} = [\mathbf{C}^{p} ; \mathbf{C}^{a} ; \mathbf{C}^{p^{\prime}} ; \mathbf{C}^{q}]$，每个选项都有对应的表达 $\mathbf{C}_i$.</p><p>loss function可以定义为：</p><script type="math/tex; mode=display">L\left(\mathbf{A}_{i} | \mathbf{P}, \mathbf{Q}\right)=-\log \frac{\exp \left(V^{T} \mathbf{C}_{i}\right)}{\sum_{j=1}^{N} \exp \left(V^{T} \mathbf{C}_{j}\right)}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整个模型的结构还是很简洁的，而且非常类似于这篇预测题目难度的paper，即<a href="https://juejin.im/post/5c979c435188252d645831ed" target="_blank" rel="noopener">Question Difﬁculty Prediction for READING Problems in Standard Tests 笔记</a>，感兴趣的可以看看那篇。两者的网络结构很相似，只不过一个拿来预测题目难度，一个拿来预测正确答案。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>DCMN在RACE数据集上取得了SOTA的结果，论文中po出的结果如下：</p><p><img src="https://user-gold-cdn.xitu.io/2019/3/27/169bd0076b7c25fd?w=1304&amp;h=868&amp;f=png&amp;s=227108" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Intro&quot;&gt;&lt;a href=&quot;#Intro&quot; class=&quot;headerlink&quot; title=&quot;Intro&quot;&gt;&lt;/a&gt;Intro&lt;/h1&gt;&lt;p&gt;目前流行的SQuAD数据集是比较简单的MRC任务，因为已经给定了阅读段落，而且问题的答案已经被完全的包含在了阅读段落
      
    
    </summary>
    
    
      <category term="BERT" scheme="https://zpqiu.github.io/tags/BERT/"/>
    
      <category term="NLP" scheme="https://zpqiu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>BERT的一些实践</title>
    <link href="https://zpqiu.github.io/BERT%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E8%B7%B5/"/>
    <id>https://zpqiu.github.io/BERT的一些实践/</id>
    <published>2019-03-27T04:36:49.278Z</published>
    <updated>2019-03-27T04:39:23.776Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读材料"><a href="#阅读材料" class="headerlink" title="阅读材料"></a>阅读材料</h1><p>关于BERT的解读有很多，我在学习的时候主要阅读了以下的几篇分享：</p><ul><li>BERT原始论文，<a href="http://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li>ELMo原始论文 <a href="http://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a></li><li><a href="http://www.xuwei.io/2018/11/20/nlp的巨人肩膀/" target="_blank" rel="noopener">nlp的巨人肩膀</a></li><li><a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">The Illustrated BERT, ELMo, and co</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">Transformer的可视化</a></li><li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li></ul><h1 id="关于Pre-train"><a href="#关于Pre-train" class="headerlink" title="关于Pre-train"></a>关于Pre-train</h1><p>作者在Github的BERT的README里提到了几点需要注意的事项：</p><ul><li>如果domain的数据集够大，可以在BERT Google model基础上进行额外的pre-training，也可以理解为对BERT原始的语言模型进行fine-tuning</li><li>BERT的paper中pre-train的lr为1e-4，如果进行额外pre-training 应该设小一点的lr，如2e-5。我自己的实验也表明了，lr=1e-4会得到比较差的结果。但是</li><li>sequences 越长带来的显存压力越大，而且是和sequence length的平方呈线性相关。</li></ul><h1 id="关于fine-tune-Google的原始语言模型"><a href="#关于fine-tune-Google的原始语言模型" class="headerlink" title="关于fine-tune Google的原始语言模型"></a>关于fine-tune Google的原始语言模型</h1><p>用Domain数据集对Google的原始语言模型进行fine-tuning，之后再训练任务的方式（即ULMFit论文采用的模式)不一定会有效果。</p><p>我的实验发现，这种方式对于小数据集有比较大的提升，但是对大数据集基本没差异。猜测，</p><ul><li>一方面，对于大数据集，BERT在训练任务的同时就完成了在Domain上的fine-tuning。</li><li>另一方面，即便是Domain相关的语料，但是其中大部分的字词都是和Domain无关的，相关的仅仅是部分实体，因此再基于这份语料训练语言模型，其实已经起不到多大的作用。</li></ul><p>还有一个方式是改造BERT里的Masked Language Model任务，BERT是随机Mask一些token来预测，但是对于中文其实是Mask了一些的字(因为BERT的谷歌模型对中文是按字来处理的)。或许我们选择Mask Domain下的一些entity来预测的方式能更好地学习Domain下的语言模型。近日，百度也放出的工作<a href="https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE" target="_blank" rel="noopener">ERNIE</a>也是采用的这种思想，其在中文多项任务上超过了BERT。</p><p>ERNIE模型的一个例子如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Learnt by BERT ：哈 [mask] 滨是 [mask] 龙江的省会，[mask] 际冰 [mask] 文化名城。</span><br><span class="line"></span><br><span class="line">Learnt by ERNIE：[mask] [mask] [mask] 是黑龙江的省会，国际 [mask] [mask] 文化名城。</span><br></pre></td></tr></table></figure></p><p>ERNIE使模型根据上下文就能推断出整个实体。当然ERNIE还引入了对话语料来改造 Next Sentence Prediction的任务。ERNIE的论文还没有放出，后续可以关注一下。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;阅读材料&quot;&gt;&lt;a href=&quot;#阅读材料&quot; class=&quot;headerlink&quot; title=&quot;阅读材料&quot;&gt;&lt;/a&gt;阅读材料&lt;/h1&gt;&lt;p&gt;关于BERT的解读有很多，我在学习的时候主要阅读了以下的几篇分享：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT原始论文，&lt;a hre
      
    
    </summary>
    
    
      <category term="BERT" scheme="https://zpqiu.github.io/tags/BERT/"/>
    
      <category term="NLP" scheme="https://zpqiu.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Learning to Ask Neural Question Generation for Reading Comprehension 笔记</title>
    <link href="https://zpqiu.github.io/learning-to-ask/"/>
    <id>https://zpqiu.github.io/learning-to-ask/</id>
    <published>2019-03-27T04:35:03.261Z</published>
    <updated>2019-03-27T05:45:41.885Z</updated>
    
    <content type="html"><![CDATA[<p>感觉和 “Neural Question Generation from Text: A Preliminary Study” 这篇很像的工作，都是基于 Seq2Seq 的思想来做 Question Generation。</p><p>两者的区别在与 Encode 和 Decode 的设计吧。</p><h2 id="Encode-和-Decode"><a href="#Encode-和-Decode" class="headerlink" title="Encode 和 Decode"></a>Encode 和 Decode</h2><p>Learning to ask 利用 Bi-LSTM 来编码，将状态拼接，在decode阶段还用于做attention。</p><p>这篇论文还对paragraph（包含答案的段落）做了encode，但是是利用了另外一个 Bi-LSTM。将最后一个状态拼接作为输出。</p><p>在Decode时使用了带attention的LSTM，几个公式如下：</p><script type="math/tex; mode=display">\mathbf { h }_{ t } = \mathbf { L S T M } _ { 1 } \left( y _ { t - 1 } , \mathbf { h }_{ t - 1 } \right)</script><script type="math/tex; mode=display">P \left( y _ { t } | \mathbf { x } , y _ { < t } \right) = \operatorname { softmax } \left( \mathbf { W } _ { s } \tanh \left( \mathbf { W } _ { t } \left[ \mathbf { h } _ { t } ; \mathbf { c } _ { t } \right] \right) \right)</script><p>其中$c_t$ 是encode-decode attention的context信息，由$h_t$和encode阶段的状态计算得出。</p><blockquote><p>论文的实现可以参考 <a href="论文code">https://github.com/xinyadu/nqg</a></p></blockquote><h2 id="NQG的-Encode-和-Decode"><a href="#NQG的-Encode-和-Decode" class="headerlink" title="NQG的 Encode 和 Decode"></a>NQG的 Encode 和 Decode</h2><p>首先输入不同，NQG输入为一段paragraph，特征也不同，输入时会标注上答案所在的位置，以及文本的<strong>词法特征</strong>。下图是整个框架的说明：</p><p><img src="https://user-gold-cdn.xitu.io/2019/3/25/169b2e0565c8c495?w=1208&amp;h=676&amp;f=png&amp;s=155548" alt="NQG框架图"></p><p>Encode的结构是<strong>Bi-GRU</strong></p><p>其次，其Decode的结构是带attention的GRU，和上篇也类似。具体的公式如下</p><script type="math/tex; mode=display">\begin{aligned} s _ { t } & = \mathrm { GRU } \left( w _ { t - 1 } , c _ { t - 1 } , s _ { t - 1 } \right) \\ s _ { 0 } & = \tanh \left( \mathbf { W } _ { d } \overline { h } _ { 1 } + b \right) \\ e _ { t , i } & = v _ { a } ^ { \top } \tanh \left( \mathbf { W } _ { a } s _ { t - 1 } + \mathbf { U } _ { a } h _ { i } \right) \\ \alpha _ { t , i } & = \frac { \exp \left( e _ { t , i } \right) } { \sum _ { i = 1 } ^ { n } \exp \left( e _ { t , i } \right) } \\ c _ { t } & = \sum _ { i = 1 } ^ { n } \alpha _ { t , i } h _ { i } \end{aligned}</script><p>其中，$h$为Encode的输出，中间状态。$s$是Decode的中间状态。在预测输出的token时，文中结合了上一单词、当前的context（与encode的attention结果）、当前状态，得到一个 <em>readout state</em> $r$，然后做了一个 <strong>maxout</strong> 的类pooling操作（没懂这一步），最后用softmax做predict。</p><script type="math/tex; mode=display">\begin{aligned} r _ { t } & = \mathbf { W } _ { r } w _ { t - 1 } + \mathbf { U } _ { r } c _ { t } + \mathbf { V } _ { r } s _ { t } \\ m _ { t } & = \left[ \max \left\{ r _ { t , 2 j - 1 } , r _ { t , 2 j } \right\} \right] _ { j = 1 , \ldots , d } ^ { \top } \\  p \left( y _ { t } | y _ { 1 } , \ldots , y _ { t - 1 } \right) = & \operatorname { softmax } \left( \mathbf { W } _ { o } m _ { t } \right) \end{aligned}</script><p>最后，NQG使用了 <strong>Copy</strong> 机制来缓解 <em>rare and unknown words</em> 的问题。在decode阶段的时刻t， 文中根据当前状态 $s_t$，和当前的context  $c_t$，来预测一个概率，是否将输入中的词copy到当前时刻，至于copy那个词，是根据attention的分数来决定。</p><script type="math/tex; mode=display">p = \sigma \left( \mathbf { W } s _ { t } + \mathbf { U } c _ { t } + b \right)</script><blockquote><p>论文的实现可以参考  <a href="论文code">https://github.com/magic282/NQG</a></p></blockquote><div class="note info">            <p>我觉着篇工作的效果应该是好于上一篇论文，但是他们之间没有对比。可能是同期的论文吧。</p>          </div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;感觉和 “Neural Question Generation from Text: A Preliminary Study” 这篇很像的工作，都是基于 Seq2Seq 的思想来做 Question Generation。&lt;/p&gt;
&lt;p&gt;两者的区别在与 Encode 和 D
      
    
    </summary>
    
    
      <category term="NLG" scheme="https://zpqiu.github.io/tags/NLG/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 的多卡并行训练</title>
    <link href="https://zpqiu.github.io/pytorch-multi-gpu/"/>
    <id>https://zpqiu.github.io/pytorch-multi-gpu/</id>
    <published>2019-03-24T07:51:00.093Z</published>
    <updated>2019-04-02T07:53:02.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h1><p>并行的方式分为了数据并行。 DataParallel 会将module复制到多个卡上，也会将每个batch均分到每张卡上，每张卡独立forward自己那份data，而在backward时，每个卡上的梯度会汇总到原始的module上，以此来实现并行。</p><p>但是，这样的方式会造成原始module在的那张卡的显存压力比其他卡要大，也就是这种方式存在负载不均衡的情况。具体情况可以看<a href="https://www.jianshu.com/p/221d9298808e" target="_blank" rel="noopener">pytorch： 一机多卡训练的尝试</a>这篇文章的实验。</p><h1 id="Multiprocessing"><a href="#Multiprocessing" class="headerlink" title="Multiprocessing"></a>Multiprocessing</h1><p>另外一种方式是利用Python的多进程，每张卡运行一个进程，每个进程有一个自己的model和一份数据，求梯度时则将多张卡的梯度汇总，然后传播到每张卡上来实现并行。</p><p>这种方式就避免了第一种方式负载不均衡的情况，而且多进程也避免了Python的GIL的机制。</p><p>但是<a href="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers" target="_blank" rel="noopener">PyTorch官方文档</a>还是推荐使用 DataParallel 的方式，其说法如下：</p><blockquote><p>Use nn.DataParallel instead of multiprocessing</p><p>Most use cases involving batched inputs and multiple GPUs should default to using DataParallel to utilize more than one GPU. Even with the GIL, a single Python process can saturate multiple GPUs.</p><p>As of version 0.1.9, large numbers of GPUs (8+) might not be fully utilized. However, this is a known issue that is under active development. As always, test your use case.</p><p>There are significant caveats to using CUDA models with multiprocessing; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p></blockquote><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>一个利用多进程实现多卡训练的样例如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(gpu_id)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    model = Model()</span><br><span class="line">    model.cuda(gpu_id)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        train(epoch, gpu_id, model, optimizer, data_loader)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> gpu_id == <span class="number">0</span>:</span><br><span class="line">            validata(...)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 汇总多卡的梯度，平均后传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_gradients</span><span class="params">(model)</span>:</span></span><br><span class="line">    size = float(dist.get_world_size())</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)</span><br><span class="line">            p.grad.data /= size</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch, gpu_id, model, optimizer, data_loader)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">        data = &#123;key: value.to(gpu_id) <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        </span><br><span class="line">        loss = model.forward(data)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> word_size &gt; <span class="number">1</span>:</span><br><span class="line">            average_gradients(model)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_process</span><span class="params">(host, port, rank, fn, backend=<span class="string">"nccl"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    host: str, 如果一机多卡可以直接填localhost</span></span><br><span class="line"><span class="string">    port: str</span></span><br><span class="line"><span class="string">    rank: int</span></span><br><span class="line"><span class="string">    fn: train的函数</span></span><br><span class="line"><span class="string">    backend: 实现多卡通信的机制，有多种，PyTorch有汇总</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = host</span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = port</span><br><span class="line">    dist.init_process_group(backend, rank=rank, world_size=world_size)</span><br><span class="line">    fn(rank)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    mp.set_start_method(<span class="string">"spawn"</span>)</span><br><span class="line">    </span><br><span class="line">    processes = []</span><br><span class="line">    <span class="comment"># 有几张卡可以指定ranks列表</span></span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> ranks:</span><br><span class="line">        p = mp.Process(target=init_process, args=(host, port, rank, run)</span><br><span class="line">        p.start()</span><br><span class="line">        processes.append(p)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure></p><p>或者直接参考官方文档 <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" target="_blank" rel="noopener">WRITING DISTRIBUTED APPLICATIONS WITH PYTORCH</a></p><h1 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h1><p>NVIDIA开发的支持并行和混合精度的辅助函数。 Github地址为：<a href="https://github.com/NVIDIA/apex" target="_blank" rel="noopener">apex</a><br>。</p><p>主要是对PyTorch多进程方式的多卡训练代码的封装，重要的是支持fp16的训练，以及混合精度的训练。</p><p>但是我还没用过apex，这里先不写了。</p><h1 id="Accumulating-gradients"><a href="#Accumulating-gradients" class="headerlink" title="Accumulating gradients"></a>Accumulating gradients</h1><p>还有一种方式是累加梯度Accumulating gradients（参考自<a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" target="_blank" rel="noopener">Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a>，作者是<a href="https://github.com/huggingface/pytorch-pretrained-BERT" target="_blank" rel="noopener">huggingface/pytorch-pretrained-BERT</a>的作者。）</p><p>方法很简单，就是假设batch_size=32，但是1个GPU的显存放不下，我们可以在forward的时候将整个batch分为8份，每次过size=4的小batch，计算小batch的梯度后进行backward，然后<strong>先不要更新参数</strong>，而是继续forward第二个小batch，计算loss，然后backward，而且将第二个小batch的梯度<strong>累加</strong>到第一个小的batch的梯度上，如此，当4个小batch都计算出梯度，再更新参数。</p><p>这样的方式在PyTorch里很容易实现，因为只有我们调用 <code>model.zero_grad()</code> 或者<code>optimizer.zero_grad()</code>时，缓存的梯度才会被重置。</p><p>Thomas Wolf给出了一个gist的例子，可以供我们参考，<br><a href="https://gist.github.com/thomwolf/3359a5e6d534b97be0cf160fb4f6bbcb" target="_blank" rel="noopener">Accumulating gradients</a>。或者我们可以直接参考<a href="https://github.com/huggingface/pytorch-pretrained-BERT" target="_blank" rel="noopener">huggingface/pytorch-pretrained-BERT</a>里的写法。</p><p><strong>Accumulating gradients</strong> 可以和分布式训练结合使用，来达到增大batch_size的作用，因为对于BERT等大模型来说，再加一些下游框架，batch_size只能设到个位数，而paper中建议的batch_size都在16或者32，太小的batch_size也会影响到模型的性能。因此我们需要更大的batch_size来稳定训练过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DataParallel&quot;&gt;&lt;a href=&quot;#DataParallel&quot; class=&quot;headerlink&quot; title=&quot;DataParallel&quot;&gt;&lt;/a&gt;DataParallel&lt;/h1&gt;&lt;p&gt;并行的方式分为了数据并行。 DataParallel 会将m
      
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://zpqiu.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Question Difﬁculty Prediction for READING Problems in Standard Tests 笔记</title>
    <link href="https://zpqiu.github.io/question-difficulty-prediction-note/"/>
    <id>https://zpqiu.github.io/question-difficulty-prediction-note/</id>
    <published>2018-12-04T15:37:55.779Z</published>
    <updated>2018-12-04T15:58:00.284Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests"><a href="#Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests" class="headerlink" title="Question Difﬁculty Prediction for READING Problems in Standard Tests"></a>Question Difﬁculty Prediction for READING Problems in Standard Tests</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li>预测难度很重要，可以帮助组题等</li><li>之前的工作基本是利用专家标注，labor intensive and subjective，带有bias</li><li>大规模积累的题目文本和学生做题log为我们提供了建立一种不用人工干预的方法的机会</li></ol><h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><ol><li>如何对文本进行语义表达</li><li>对不同考试下的不同题目的难度（错答率）对比没有意义，需要找到一种方法来训练模型</li></ol><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><p>框架图如下所示：<br><img src="https://lh3.googleusercontent.com/-ndMkRYXVJsY/XAafuquCyfI/AAAAAAAAIMk/B9Hwv0_ldioAaVVpxg1sdJBVask6McvNQCHMYCw/I/15439171873234.jpg" alt="-w1295"></p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol><li>题目的文本材料，以句子为单元，句子由词组成</li><li>学生的做题log</li></ol><h3 id="Sentence-CNN-Layer"><a href="#Sentence-CNN-Layer" class="headerlink" title="Sentence CNN Layer"></a>Sentence CNN Layer</h3><p>这部分是通过CNN来encode文本中的句子。CNN能从local到global的捕捉文本中的语义信息。文中采用的 wide-CNN 和 p-max pooling 结合的方式。</p><p><em>不是很清楚为啥采用这种CNN结构，而不是如同textCNN一样的进行2-D卷积。</em></p><p><strong>CNN的操作是：</strong></p><p>将连续k个words的embedding拼接，然后与权重矩阵相同得到新的隐层向量。</p><script type="math/tex; mode=display">\vec { h } _ { i } ^ { c } = \sigma \left( \mathbf { G } \cdot \left[ w _ { i - k + 1 } \oplus \cdots \oplus w _ { i } \right] + \mathbf { b } \right)</script><p><strong>Pooling的操作是：</strong><br>将连续k个word的按维度进行pool， 公式如下：</p><script type="math/tex; mode=display">\vec { h } _ { i } ^ { c p } = \left[ \max \left[ \begin{array} { c } { h _ { i - p + 1,1 } ^ { c } } \\ { \dot { h _ { i , 1 } ^ { c } } } \end{array} \right] , \cdots , \max \left[ \begin{array} { c } { h _ { i - p + 1 , d } ^ { c } } \\ { h _ { i , d } ^ { c } } \end{array} \right] \right]</script><p>整个示意图如下，经过多个卷积层最终将sentence编码成等长的向量<br><img src="https://lh3.googleusercontent.com/-IREQwgoaUHs/XAaft18O2II/AAAAAAAAIMg/yrdQ7dicYa8_b74pj_QG6FKghnF-RUspgCHMYCw/I/15439174934168.jpg" alt="-w676"></p><h3 id="Attention-Layer"><a href="#Attention-Layer" class="headerlink" title="Attention Layer"></a>Attention Layer</h3><p>这一层是将阅读材料和选项中的句子表示和问题表示做attention操作，之后得到阅读材料和选项的attentive的表达。</p><p>作者的解释是同一个材料对于不同的question的应该形成不同的表达。这一步应该是来对不同句子对解答问题做的贡献进行qualify？ 感觉这个解释都不是很直觉。</p><p>attention操作文中使用的是cosine函数。</p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>这部分是文中的一个重点。文中首先在Figure 1(b)中展示了两道题目在不同的测试中的错答率是不同的，以此说明了不同题目在不同测验下得到的难度是不具有可比性的。因此不能直接将其拿来和predict的难度做loss。</p><p>作者观察到在同一个测验下会有多个题目，而同一测验代表了同一组学生，因此同一测验下的题目难度之间是有可比性的，以此作者提出了 test-dependent pairwise 训练策略。loss function的形式如下：</p><script type="math/tex; mode=display">\mathcal { J } ( \Theta ) = \sum _ { \left( T _ { t } , Q _ { i } , Q _ { j } \right) } \left( \left( P _ { i } ^ { t } - P _ { j } ^ { t } \right) - \left( \mathcal { M } \left( Q _ { i } \right) - \mathcal { M } \left( Q _ { j } \right) \right) \right) ^ { 2 } + \lambda _ { \Theta } \left\| \Theta _ { \mathcal { M } } \right\| ^ { 2 }</script><p>通过拟合同一test下两道题目的difficulty的差值来训练网络中的参数。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>文中提到的数据集不是公开数据集。数据描述也有冲突（log数文中提到接近300万，表格中是2800万，不知道是多打了哪一位数），可能pdf是pre-print的吧。</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>用的word2vec的预训练结果，OOV则random</p><h3 id="模型setting"><a href="#模型setting" class="headerlink" title="模型setting"></a>模型setting</h3><div class="table-container"><table><thead><tr><th>Sentence sequence length</th><th>25</th></tr></thead><tbody><tr><td>Word sequence length</td><td>40</td></tr><tr><td>Feature map</td><td>200, 400, 600, 600</td></tr><tr><td>Kernel size(k)</td><td>3</td></tr><tr><td>Pool size(p)</td><td>3, 3, 2, 1</td></tr><tr><td>batch size</td><td>32</td></tr><tr><td>dropout</td><td>0.2</td></tr></tbody></table></div><h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><ol><li>RMSE</li><li>degree of agreement (DOA) ??</li><li>person 相关系数</li><li>t-test passing ratio ？？ </li></ol><p>本文还和专家标注结果进行了对比。</p><p>挑选了12个测试中的4个阅读材料16到题目，以及7位专家进行评测。</p><p>然后将模型结果以及专家标注结果分别和真值(错答率)进行计算相关系数。结果表明有的模型由于所有的专家。也说明了专家有bias</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests&quot;&gt;&lt;a href=&quot;#Question-Difﬁculty-Prediction-for-READING-Problems-i
      
    
    </summary>
    
    
      <category term="Paper" scheme="https://zpqiu.github.io/tags/Paper/"/>
    
      <category term="Education" scheme="https://zpqiu.github.io/tags/Education/"/>
    
  </entry>
  
  <entry>
    <title>hello-page</title>
    <link href="https://zpqiu.github.io/hello-page/"/>
    <id>https://zpqiu.github.io/hello-page/</id>
    <published>2018-12-04T15:30:54.000Z</published>
    <updated>2018-12-04T15:30:54.656Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zpqiu.github.io/hello-world/"/>
    <id>https://zpqiu.github.io/hello-world/</id>
    <published>2018-12-04T15:01:05.835Z</published>
    <updated>2018-12-04T15:01:05.835Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
