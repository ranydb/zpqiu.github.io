<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Alex Chiu的学习空间</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zpqiu.github.io/"/>
  <updated>2019-03-24T08:38:50.231Z</updated>
  <id>https://zpqiu.github.io/</id>
  
  <author>
    <name>Alex Chiu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch 的多卡并行训练</title>
    <link href="https://zpqiu.github.io/2019/03/24/pytorch-multi-gpu/"/>
    <id>https://zpqiu.github.io/2019/03/24/pytorch-multi-gpu/</id>
    <published>2019-03-24T07:51:00.093Z</published>
    <updated>2019-03-24T08:38:50.231Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h1><p>并行的方式分为了数据并行。 DataParallel 会将module复制到多个卡上，也会将每个batch均分到每张卡上，每张卡独立forward自己那份data，而在backward时，每个卡上的梯度会汇总到原始的module上，以此来实现并行。</p><p>但是，这样的方式会造成原始module在的那张卡的显存压力比其他卡要大，也就是这种方式存在负载不均衡的情况。具体情况可以看<a href="https://www.jianshu.com/p/221d9298808e" target="_blank" rel="noopener">pytorch： 一机多卡训练的尝试</a>这篇文章的实验。</p><h1 id="Multiprocessing"><a href="#Multiprocessing" class="headerlink" title="Multiprocessing"></a>Multiprocessing</h1><p>另外一种方式是利用Python的多进程，每张卡运行一个进程，每个进程有一个自己的model和一份数据，求梯度时则将多张卡的梯度汇总，然后传播到每张卡上来实现并行。</p><p>这种方式就避免了第一种方式负载不均衡的情况，而且多进程也避免了Python的GIL的机制。</p><p>但是<a href="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers" target="_blank" rel="noopener">PyTorch官方文档</a>还是推荐使用 DataParallel 的方式，其说法如下：</p><blockquote><p>Use nn.DataParallel instead of multiprocessing</p><p>Most use cases involving batched inputs and multiple GPUs should default to using DataParallel to utilize more than one GPU. Even with the GIL, a single Python process can saturate multiple GPUs.</p><p>As of version 0.1.9, large numbers of GPUs (8+) might not be fully utilized. However, this is a known issue that is under active development. As always, test your use case.</p><p>There are significant caveats to using CUDA models with multiprocessing; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p></blockquote><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>一个利用多进程实现多卡训练的样例如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(gpu_id)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    model = Model()</span><br><span class="line">    model.cuda(gpu_id)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        train(epoch, gpu_id, model, optimizer, data_loader)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> gpu_id == <span class="number">0</span>:</span><br><span class="line">            validata(...)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 汇总多卡的梯度，平均后传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_gradients</span><span class="params">(model)</span>:</span></span><br><span class="line">    size = float(dist.get_world_size())</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)</span><br><span class="line">            p.grad.data /= size</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch, gpu_id, model, optimizer, data_loader)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">        data = &#123;key: value.to(gpu_id) <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        </span><br><span class="line">        loss = model.forward(data)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> word_size &gt; <span class="number">1</span>:</span><br><span class="line">            average_gradients(model)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_process</span><span class="params">(host, port, rank, fn, backend=<span class="string">"nccl"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    host: str, 如果一机多卡可以直接填localhost</span></span><br><span class="line"><span class="string">    port: str</span></span><br><span class="line"><span class="string">    rank: int</span></span><br><span class="line"><span class="string">    fn: train的函数</span></span><br><span class="line"><span class="string">    backend: 实现多卡通信的机制，有多种，PyTorch有汇总</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = host</span><br><span class="line">    os.environ[<span class="string">"MASTER_ADDR"</span>] = port</span><br><span class="line">    dist.init_process_group(backend, rank=rank, world_size=world_size)</span><br><span class="line">    fn(rank)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    mp.set_start_method(<span class="string">"spawn"</span>)</span><br><span class="line">    </span><br><span class="line">    processes = []</span><br><span class="line">    <span class="comment"># 有几张卡可以指定ranks列表</span></span><br><span class="line">    <span class="keyword">for</span> rank <span class="keyword">in</span> ranks:</span><br><span class="line">        p = mp.Process(target=init_process, args=(host, port, rank, run)</span><br><span class="line">        p.start()</span><br><span class="line">        processes.append(p)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure></p><p>或者直接参考官方文档 <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html" target="_blank" rel="noopener">WRITING DISTRIBUTED APPLICATIONS WITH PYTORCH</a></p><h1 id="Apex"><a href="#Apex" class="headerlink" title="Apex"></a>Apex</h1><p>NVIDIA开发的支持并行和混合精度的辅助函数。 Github地址为：<a href="https://github.com/NVIDIA/apex" target="_blank" rel="noopener">apex</a><br>。</p><p>主要是对PyTorch多进程方式的多卡训练代码的封装，重要的是支持fp16的训练，以及混合精度的训练。</p><p>但是我还没用过apex，这里先不写了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DataParallel&quot;&gt;&lt;a href=&quot;#DataParallel&quot; class=&quot;headerlink&quot; title=&quot;DataParallel&quot;&gt;&lt;/a&gt;DataParallel&lt;/h1&gt;&lt;p&gt;并行的方式分为了数据并行。 DataParallel 会将m
      
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://zpqiu.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Question Difﬁculty Prediction for READING Problems in Standard Tests 笔记</title>
    <link href="https://zpqiu.github.io/2018/12/04/question-difficulty-prediction-note/"/>
    <id>https://zpqiu.github.io/2018/12/04/question-difficulty-prediction-note/</id>
    <published>2018-12-04T15:37:55.779Z</published>
    <updated>2018-12-04T15:58:00.284Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests"><a href="#Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests" class="headerlink" title="Question Difﬁculty Prediction for READING Problems in Standard Tests"></a>Question Difﬁculty Prediction for READING Problems in Standard Tests</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li>预测难度很重要，可以帮助组题等</li><li>之前的工作基本是利用专家标注，labor intensive and subjective，带有bias</li><li>大规模积累的题目文本和学生做题log为我们提供了建立一种不用人工干预的方法的机会</li></ol><h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><ol><li>如何对文本进行语义表达</li><li>对不同考试下的不同题目的难度（错答率）对比没有意义，需要找到一种方法来训练模型</li></ol><h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><p>框架图如下所示：<br><img src="https://lh3.googleusercontent.com/-ndMkRYXVJsY/XAafuquCyfI/AAAAAAAAIMk/B9Hwv0_ldioAaVVpxg1sdJBVask6McvNQCHMYCw/I/15439171873234.jpg" alt="-w1295"></p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol><li>题目的文本材料，以句子为单元，句子由词组成</li><li>学生的做题log</li></ol><h3 id="Sentence-CNN-Layer"><a href="#Sentence-CNN-Layer" class="headerlink" title="Sentence CNN Layer"></a>Sentence CNN Layer</h3><p>这部分是通过CNN来encode文本中的句子。CNN能从local到global的捕捉文本中的语义信息。文中采用的 wide-CNN 和 p-max pooling 结合的方式。</p><p><em>不是很清楚为啥采用这种CNN结构，而不是如同textCNN一样的进行2-D卷积。</em></p><p><strong>CNN的操作是：</strong></p><p>将连续k个words的embedding拼接，然后与权重矩阵相同得到新的隐层向量。</p><script type="math/tex; mode=display">\vec { h } _ { i } ^ { c } = \sigma \left( \mathbf { G } \cdot \left[ w _ { i - k + 1 } \oplus \cdots \oplus w _ { i } \right] + \mathbf { b } \right)</script><p><strong>Pooling的操作是：</strong><br>将连续k个word的按维度进行pool， 公式如下：</p><script type="math/tex; mode=display">\vec { h } _ { i } ^ { c p } = \left[ \max \left[ \begin{array} { c } { h _ { i - p + 1,1 } ^ { c } } \\ { \dot { h _ { i , 1 } ^ { c } } } \end{array} \right] , \cdots , \max \left[ \begin{array} { c } { h _ { i - p + 1 , d } ^ { c } } \\ { h _ { i , d } ^ { c } } \end{array} \right] \right]</script><p>整个示意图如下，经过多个卷积层最终将sentence编码成等长的向量<br><img src="https://lh3.googleusercontent.com/-IREQwgoaUHs/XAaft18O2II/AAAAAAAAIMg/yrdQ7dicYa8_b74pj_QG6FKghnF-RUspgCHMYCw/I/15439174934168.jpg" alt="-w676"></p><h3 id="Attention-Layer"><a href="#Attention-Layer" class="headerlink" title="Attention Layer"></a>Attention Layer</h3><p>这一层是将阅读材料和选项中的句子表示和问题表示做attention操作，之后得到阅读材料和选项的attentive的表达。</p><p>作者的解释是同一个材料对于不同的question的应该形成不同的表达。这一步应该是来对不同句子对解答问题做的贡献进行qualify？ 感觉这个解释都不是很直觉。</p><p>attention操作文中使用的是cosine函数。</p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>这部分是文中的一个重点。文中首先在Figure 1(b)中展示了两道题目在不同的测试中的错答率是不同的，以此说明了不同题目在不同测验下得到的难度是不具有可比性的。因此不能直接将其拿来和predict的难度做loss。</p><p>作者观察到在同一个测验下会有多个题目，而同一测验代表了同一组学生，因此同一测验下的题目难度之间是有可比性的，以此作者提出了 test-dependent pairwise 训练策略。loss function的形式如下：</p><script type="math/tex; mode=display">\mathcal { J } ( \Theta ) = \sum _ { \left( T _ { t } , Q _ { i } , Q _ { j } \right) } \left( \left( P _ { i } ^ { t } - P _ { j } ^ { t } \right) - \left( \mathcal { M } \left( Q _ { i } \right) - \mathcal { M } \left( Q _ { j } \right) \right) \right) ^ { 2 } + \lambda _ { \Theta } \left\| \Theta _ { \mathcal { M } } \right\| ^ { 2 }</script><p>通过拟合同一test下两道题目的difficulty的差值来训练网络中的参数。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>文中提到的数据集不是公开数据集。数据描述也有冲突（log数文中提到接近300万，表格中是2800万，不知道是多打了哪一位数），可能pdf是pre-print的吧。</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>用的word2vec的预训练结果，OOV则random</p><h3 id="模型setting"><a href="#模型setting" class="headerlink" title="模型setting"></a>模型setting</h3><div class="table-container"><table><thead><tr><th>Sentence sequence length</th><th>25</th></tr></thead><tbody><tr><td>Word sequence length</td><td>40</td></tr><tr><td>Feature map</td><td>200, 400, 600, 600</td></tr><tr><td>Kernel size(k)</td><td>3</td></tr><tr><td>Pool size(p)</td><td>3, 3, 2, 1</td></tr><tr><td>batch size</td><td>32</td></tr><tr><td>dropout</td><td>0.2</td></tr></tbody></table></div><h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><ol><li>RMSE</li><li>degree of agreement (DOA) ??</li><li>person 相关系数</li><li>t-test passing ratio ？？ </li></ol><p>本文还和专家标注结果进行了对比。</p><p>挑选了12个测试中的4个阅读材料16到题目，以及7位专家进行评测。</p><p>然后将模型结果以及专家标注结果分别和真值(错答率)进行计算相关系数。结果表明有的模型由于所有的专家。也说明了专家有bias</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Question-Difﬁculty-Prediction-for-READING-Problems-in-Standard-Tests&quot;&gt;&lt;a href=&quot;#Question-Difﬁculty-Prediction-for-READING-Problems-i
      
    
    </summary>
    
    
      <category term="Paper" scheme="https://zpqiu.github.io/tags/Paper/"/>
    
      <category term="Education" scheme="https://zpqiu.github.io/tags/Education/"/>
    
  </entry>
  
  <entry>
    <title>hello-page</title>
    <link href="https://zpqiu.github.io/2018/12/04/hello-page/"/>
    <id>https://zpqiu.github.io/2018/12/04/hello-page/</id>
    <published>2018-12-04T15:30:54.000Z</published>
    <updated>2018-12-04T15:30:54.656Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zpqiu.github.io/2018/12/04/hello-world/"/>
    <id>https://zpqiu.github.io/2018/12/04/hello-world/</id>
    <published>2018-12-04T15:01:05.835Z</published>
    <updated>2018-12-04T15:01:05.835Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
